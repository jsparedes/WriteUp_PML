{
    "contents" : "\n# Required libraries ------------------------------------------------------\n\npackages = c(\"mi\", \"caret\")  # Here, other tabplotd3           \n\nipak <- function(pkg){                                          \n  new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])] \n  if (length(new.pkg))                                          \n    install.packages(new.pkg, dependencies = TRUE)              \n  sapply(pkg, require, character.only = TRUE)                   \n}\n\nipak(packages)\n\n# Reading datasets --------------------------------------------------------\n\nurl_training = \"pml-training.csv\"\ntraining = read.csv(url_training, sep=\",\", header = TRUE, na.strings= c(\"NA\",\"\",\" \"), nrow=19622)\n\nurl_testing = \"pml-testing.csv\"\ntesting = read.csv(url_testing, sep=\",\", header = TRUE, na.strings= c(\"NA\",\"\",\" \"), nrow=20)\n\n# In the first instance, are removed attributes related to: timestam, X, usern_name, new_window\nremoveIndex = grep(\"timestamp|X|user_name|new_window\",names(training));\ntraining = training[,-removeIndex];\ntesting = testing[,-removeIndex];\n\nsummary(training)\n\n# function de mi package\nmissing.pattern.plot(training[1:200,2:80], ylab = \"\")\n\n# We can see many missing values with a curious pattern: \n# attributes which have missing values present exactly 19216 each one\n# If we remove registers with missing values we will suffer an enormous decreasing\n# from 19622 to 406. For this reason, we only delete the column attribute \n# which present NA (missing values).\n# Doing this, we got a new dataset: [19216x53]\n# Namely, nrow(training)-sum(c) attributes left out\n\na = colSums(is.na(training))\nc = a!= 19216 # index of columnas with 19216 missing values each one\ntraining = training[, c]\ntesting = testing[, c]\n\n# Esta cantidad de columnas quedan fuera de analisis: \n\nsummary(training) # Ya no observamos missing values\n\n# Atributos a explorar:\nnames(training)\n\n \n\n# Split into Train & Validation -------------------------------------------\n# In this part we are going to split training dataset into Train and Validation\n# to evaluate the performance in some models\n\nset.seed(625)\n\ntrainIndex = createDataPartition(training$classe, p = .6,\n                                  list = FALSE,\n                                  times = 1)\n\ntrain = training[trainIndex, ]\nvalidation = training[-trainIndex, ]\n\n# Model Fitting -----------------------------------------------------------\n\nrpartModel = train(classe ~ .,\n                   data = train,\n                   method = \"rpart\",\n                   tuneLength = 30)\n\nctreeModel = train(classe ~ .,\n                   data = train,\n                   method = \"ctree\",\n                   tuneLength = 10)\n\ntreebagModel = train(classe ~ .,\n                     data = train,\n                     method = \"treebag\")\n\nlibrary(caret)\n\ntrControl = trainControl(method = \"cv\", number = 2, allowParallel =TRUE)\nmodFitRF = train(classe ~.,data = training,method=\"rf\",trControl=trControl)\n\npredictedValues = predict(modFitRF,testing);\nView(predictedValues);\n\npRes = postResample(predictedValues, testing$classe)\ncfM = confusionMatrix(predictedValues, testing$classe)\n\n# Save predicted values\nanswers = predictedValues\n",
    "created" : 1414161720683.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3881876214",
    "id" : "FAD8295E",
    "lastKnownWriteTime" : 1414273508,
    "path" : "E:/Jorge/moocs/Practical ML/WriteUp/PWriteUp.r",
    "project_path" : "PWriteUp.r",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}